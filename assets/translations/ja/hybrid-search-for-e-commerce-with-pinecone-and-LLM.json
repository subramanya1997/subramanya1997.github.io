{
  "title": "PineconeとLLMを使用したEコマース向けハイブリッド検索",
  "excerpt": "従来の情報検索手法と言語モデル(LLM)やマネージド型ベクトルデータベースであるPineconeなどの機械学習モデルを組み合わせることで、Eコマースアプリケーション向けの強力なハイブリッド検索システムを構築する方法を学びます。検索関連性の向上、パーソナライゼーション、ロングテールクエリの処理、インフラストラクチャ管理の簡素化など、Eコマースにおけるハイブリッド検索のメリットを発見してください。",
  "content_html": "<p>関連性の高い商品を検索して見つけることは、Eコマースウェブサイトの重要な要素です。高速で正確な検索結果を提供することは、高いユーザー満足度とユーザーのフラストレーションの違いを生み出します。自然言語理解とベクトル検索技術の最近の進歩により、強化された検索システムはよりアクセスしやすく効率的になり、より良いユーザー体験とコンバージョン率の向上につながっています。</p>\n\n<p>このブログ記事では、高性能ベクトル検索エンジンであるPineconeと、ドメイン固有にファインチューニングされた言語モデルを使用して、Eコマース向けのハイブリッド検索システムを実装する方法を探ります。この記事の終わりまでに、ハイブリッド検索の強固な理解だけでなく、それを実装するための実践的なステップバイステップガイドも得られます。</p>\n\n<h2>ハイブリッド検索とは?</h2>\n\n<img src=\"/assets/images/pinecone_hybrid_index.jpg\" alt=\"Pinecone Hybrid Index\" class=\"post-img\" width=\"2360\" height=\"921\" />\n<span class=\"post-img-caption\">シンプルなPinecone Hybrid Indexの高レベルビュー</span>\n\n<p>実装に入る前に、ハイブリッド検索が何を意味するのかを簡単に理解しましょう。ハイブリッド検索は、従来の検索(スパースベクトル検索)とベクトル検索(デンスベクトル検索)の両方の強みを組み合わせて、幅広いドメインでより良い検索パフォーマンスを実現するアプローチです。</p>\n\n<p>デンスベクトル検索は、テキストデータから高品質なベクトル埋め込みを抽出し、類似性検索を実行して関連するドキュメントを見つけます。しかし、ドメイン固有のデータセットでファインチューニングされていない場合、ドメイン外のデータに苦労することがよくあります。</p>\n\n<p>一方、従来の検索は、TF-IDF(単語頻度-逆文書頻度)やBM25などのスパースベクトル表現を使用し、ドメイン固有のファインチューニングを必要としません。新しいドメインを処理できますが、単語間の意味的関係を理解できないため、パフォーマンスが制限され、デンス検索の知能性に欠けています。</p>\n\n<p>ハイブリッド検索は、両方のアプローチを単一のシステムに組み合わせることで、両方のアプローチの弱点を軽減しようとし、デンスベクトル検索のパフォーマンスポテンシャルと従来の検索のゼロショット適応性を活用します。</p>\n\n<p>ハイブリッド検索の基本的な理解ができたので、その実装に入りましょう。</p>\n\n<h2>ハイブリッド検索システムの構築</h2>\n\n<p>ハイブリッド検索システムを実装するための以下のステップをカバーします:</p>\n\n<ol>\n<li>ドメイン固有言語モデルの活用</li>\n<li>スパースベクトルとデンスベクトルの作成</li>\n<li>Pineconeのセットアップ</li>\n<li>ハイブリッド検索パイプラインの実装</li>\n<li>クエリの実行とパラメータのチューニング</li>\n</ol>\n\n<h3>1. ドメイン固有言語モデルの活用</h3>\n\n<p>近年、OpenAIのGPTやCohereなどの大規模事前学習済み言語モデルは、自然言語理解や生成を含むさまざまなタスクでますます人気が高まっています。これらのモデルは、ドメイン固有のデータでファインチューニングして、パフォーマンスを向上させ、Eコマース商品検索などの特定のタスクに適応させることができます。</p>\n\n<p>この例では、ファインチューニングされたドメイン固有言語モデルを使用して、商品とクエリのデンスベクトル埋め込みを生成します。ただし、他のモデルを選択したり、特定のドメインに基づいて独自のカスタム埋め込みを作成したりすることもできます。</p>\n\n<pre><code class=\"language-python\">import torch\nfrom transformers import AutoTokenizer, AutoModel\n\n# ドメイン固有の事前学習済み言語モデルをロード\nmodel_name = \"your-domain-specific-model\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\n\n# 商品説明のデンスベクトル埋め込みを生成\ntext = \"Nike Air Max sports shoes for men\"\ninputs = tokenizer(text, return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model(**inputs)\n    dense_embedding = outputs.last_hidden_state.mean(dim=1).numpy()\n</code></pre>\n\n<h3>2. スパースベクトルとデンスベクトルの作成</h3>\n\n<p>ハイブリッド検索には、Eコマースデータのスパースベクトル表現とデンスベクトル表現の両方が必要です。これらのベクトルを生成する方法について説明します。</p>\n\n<h4>スパースベクトル</h4>\n\n<p>TF-IDFやBM25などのスパースベクトル表現は、トークン化、ストップワード除去、ステミングなどの標準的なテキスト処理技術を使用して作成できます。スパースベクトルを生成する例は、語彙行列を使用して実現できます。</p>\n\n<pre><code class=\"language-python\"># この関数は商品説明のリストのスパースベクトル表現を生成します\ndef generate_sparse_vectors(text):\n    '''商品説明のリストのスパースベクトル表現を生成\n\n    Args:\n        text (list): 商品説明のリスト\n\n    Returns:\n        sparse_vector (dict): インデックスと値の辞書\n    '''\n    sparse_vector = bm25.encode_queries(text)\n    return sparse_vector\n\nfrom pinecone_text.sparse import BM25Encoder\n\n# BM25エンコーダーを作成してデータをフィット\nbm25 = BM25Encoder()\nbm25.fit(new_df.full_data)\n\n# スパースベクトルを作成\nsparse_vectors = []\nfor product_description in product_descriptions:\n    sparse_vectors.append(generate_sparse_vectors(text=product_description))\n</code></pre>\n\n<h4>デンスベクトル</h4>\n\n<p>デンスベクトル表現は、事前学習済みまたはカスタムのドメイン固有言語モデルを使用して生成できます。前の例では、ドメイン固有言語モデルを使用して商品説明のデンスベクトル埋め込みを生成しました。</p>\n\n<pre><code class=\"language-python\">def generate_dense_vector(text):\n    '''商品説明のリストのデンスベクトル埋め込みを生成\n\n    Args:\n        text (list): 商品説明のリスト\n\n    Returns:\n        dense_embedding (np.array): デンスベクトル埋め込みのnumpy配列\n    '''\n    # テキストをトークン化してPyTorchテンソルに変換\n    inputs = tokenizer(text, return_tensors=\"pt\")\n    # 事前学習済みモデルで埋め込みを生成\n    with torch.no_grad():\n        outputs = model(**inputs)\n        dense_vector = outputs.last_hidden_state.mean(dim=1).numpy()\n    return dense_vector\n\n# 商品説明のリストのデンスベクトル埋め込みを生成\ndense_vectors = []\nfor product_description in product_descriptions:\n    dense_vectors.append(generate_dense_vector(text=product_description))\n</code></pre>\n\n<h3>3. Pineconeのセットアップ</h3>\n\n<p>Pineconeは、ハイブリッド検索をサポートする高性能ベクトル検索エンジンです。スパースベクトルとデンスベクトルの両方に対して単一のインデックスを作成でき、異なるデータモダリティ間での検索クエリをシームレスに処理します。</p>\n\n<p>Pineconeを使用するには、アカウントにサインアップし、Pineconeクライアントをインストールし、APIキーと環境を設定する必要があります。</p>\n\n<pre><code class=\"language-python\"># Pineconeハイブリッド検索インデックスを作成\nimport pinecone\n\npinecone.init(\n    api_key=\"YOUR_API_KEY\",  # app.pinecone.io\n    environment=\"YOUR_ENV\"  # コンソールのAPIキーの隣にあります\n)\n\n# Pineconeハイブリッド検索インデックスを作成\nindex_name = \"ecommerce-hybrid-search\"\npinecone.create_index(\n    index_name = index_name,\n    dimension = MODEL_DIMENSION,  # デンスモデルの次元数\n    metric = \"dotproduct\"\n)\n# インデックスに接続\nindex = pinecone.Index(index_name=index_name)\n# インデックス統計を表示\nindex.describe_index_stats()\n</code></pre>\n\n<h3>4. ハイブリッド検索パイプラインの実装</h3>\n\n<p>スパースベクトルとデンスベクトルが生成され、Pineconeがセットアップされたので、ハイブリッド検索パイプラインを構築できます。このパイプラインには以下のステップが含まれます:</p>\n\n<ol>\n<li>Pineconeインデックスへの商品データの追加</li>\n<li>スパースベクトルとデンスベクトルの両方を使用した結果の取得</li>\n</ol>\n\n<pre><code class=\"language-python\">def add_product_data_to_index(product_ids, sparse_vectors, dense_vectors, metadata=None):\n    \"\"\"商品データをPineconeインデックスにアップサート。\n\n    Args:\n        product_ids (`list` of `str`): 商品ID。\n        sparse_vectors (`list` of `list` of `float`): スパースベクトル。\n        dense_vectors (`list` of `list` of `float`): デンスベクトル。\n        metadata (`list` of `list` of `str`): オプションのメタデータ。\n\n    Returns:\n        None\n    \"\"\"\n    batch_size = 32\n\n    # 商品IDをバッチでループ。\n    for i in range(0, len(product_ids), batch_size):\n        i_end = min(i + batch_size, len(product_ids))\n        ids = product_ids[i:i_end]\n        sparse_batch = sparse_vectors[i:i_end]\n        dense_batch = dense_vectors[i:i_end]\n        meta_batch = metadata[i:i_end] if metadata else []\n\n        vectors = []\n        for _id, sparse, dense, meta in zip(ids, sparse_batch, dense_batch, meta_batch):\n            vectors.append({\n                'id': _id,\n                'sparse_values': sparse,\n                'values': dense,\n                'metadata': meta\n            })\n\n        # ベクトルをPineconeインデックスにアップサート。\n        index.upsert(vectors=vectors)\n\nadd_product_data_to_index(product_ids, sparse_vectors, dense_vectors)\n</code></pre>\n\n<p>データがインデックス化されたので、ハイブリッド検索クエリを実行できます。</p>\n\n<h3>5. クエリの実行とパラメータのチューニング</h3>\n\n<img src=\"/assets/images/pinecone_hybrid_query.jpg\" alt=\"Pinecone Hybrid Query\" class=\"post-img\" width=\"2360\" height=\"892\" />\n<span class=\"post-img-caption\">シンプルなPinecone Hybrid Queryの高レベルビュー</span>\n\n<p>ハイブリッド検索クエリを実行するために、クエリ、上位結果の数、デンスベクトル検索とスパースベクトル検索のスコア間の重み付けを制御するalphaパラメータを受け取る関数を作成します。</p>\n\n<pre><code class=\"language-python\">def hybrid_scale(dense, sparse, alpha: float):\n    \"\"\"凸結合を使用したハイブリッドベクトルスケーリング\n\n    alpha * dense + (1 - alpha) * sparse\n\n    Args:\n        dense: floatの配列\n        sparse: `indices`と`values`の辞書\n        alpha: 0と1の間のfloat。0はスパースのみ、\n               1はデンスのみを意味します\n    \"\"\"\n    if alpha &lt; 0 or alpha &gt; 1:\n        raise ValueError(\"Alphaは0と1の間でなければなりません\")\n    # スパースベクトルとデンスベクトルをスケーリングしてハイブリッド検索ベクトルを作成\n    hsparse = {\n        'indices': sparse['indices'],\n        'values':  [v * (1 - alpha) for v in sparse['values']]\n    }\n    hdense = [v * alpha for v in dense]\n    return hdense, hsparse\n\ndef search_products(query, top_k=10, alpha=0.5):\n    # スパースクエリベクトルを生成\n    sparse_query_vector = generate_sparse_vector(query)\n\n    # デンスクエリベクトルを生成\n    dense_query_vector = generate_dense_vector(query)\n\n    # ハイブリッドクエリベクトルを計算\n    dense_query_vector, sparse_query_vector = hybrid_scale(dense_query_vector, sparse_query_vector, alpha)\n\n    # Pineconeを使用して商品を検索\n    results = index.query(\n        vector=dense_query_vector,\n        sparse_vector=sparse_query_vector,\n        top_k=top_k\n    )\n\n    return results\n</code></pre>\n\n<p>この関数を使用して、Eコマースデータセット内の関連商品を検索できます。</p>\n\n<pre><code class=\"language-python\">query = \"running shoes for women\"\nresults = search_products(query, top_k=5)\n\nfor result in results:\n    print(result['id'], result['metadata']['product_name'], result['score'])\n</code></pre>\n\n<p>alphaパラメータのさまざまな値を試すことで、特定のドメインに対するスパースベクトル検索とデンスベクトル検索の最適なバランスを見つけることができます。</p>\n\n<h2>まとめ</h2>\n\n<p>このブログ記事では、Pineconeとドメイン固有言語モデルを使用してEコマース向けのハイブリッド検索システムを構築する方法を実演しました。ハイブリッド検索により、従来の検索とベクトル検索の両方の強みを組み合わせることができ、多様なドメインにわたって検索パフォーマンスと適応性が向上します。</p>\n\n<p>この記事で提供されているステップとコードスニペットに従うことで、Eコマースウェブサイトの特定の要件に合わせた独自のハイブリッド検索システムを実装できます。Pineconeの探索を始めて、今日からEコマース検索体験を向上させましょう!</p>\n\n<h2>参考文献</h2>\n\n<ul>\n<li><a href=\"https://colab.research.google.com/github/pinecone-io/examples/blob/master/search/hybrid-search/ecommerce-search/ecommerce-search.ipynb\">Pineconeのハイブリッド検索技術を使用したEコマース検索(Google Colabノートブック)</a>: Pineconeのハイブリッド検索技術を使用したEコマース検索の実装を紹介する実践的なガイド。</li>\n<li><a href=\"https://docs.pinecone.io/docs/ecommerce-search\">Pinecone Eコマース検索ドキュメント</a>: Eコマース検索システムを構築するための公式Pineconeドキュメント。</li>\n<li><a href=\"https://colab.research.google.com/github/pinecone-io/examples/blob/master/pinecone/sparse/bm25/bm25-vector-generation.ipynb\">Pineconeを使用したBM25ベクトル生成(Google Colabノートブック)</a>: Pineconeを使用してBM25スパースベクトルを生成するためのガイド。</li>\n<li><a href=\"https://github.com/pinecone-io/pinecone-text\">GitHubのPinecone Textリポジトリ</a>: Pineconeを使用したテキスト処理とベクトル生成リソースのコレクション。</li>\n<li><a href=\"https://www.pinecone.io/learn/hybrid-search-intro/\">Pineconeウェブサイトのハイブリッド検索入門</a>: Pineconeの機能の文脈におけるハイブリッド検索、そのメリット、ユースケースの概要。</li>\n</ul>",
  "source_hash": "sha256:c8b3789c888127b9f8404365e651e80624c4177f346d2aef54a3b185e2cd138b",
  "model": "claude-sonnet-4-5-20250929",
  "generated_at": "2026-01-15T20:04:32.931748+00:00"
}